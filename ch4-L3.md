# ۲۲ Arrow

## ۱.۲۲ مقدمه

فایل‌های CSV برای خوانده شدن توسط انسان‌ها طراحی شده‌اند. آن‌ها یک فرمت تبادل خوب هستند زیرا بسیار ساده هستند و می‌توانند توسط هر ابزاری در دنیا خوانده شوند. اما فایل‌های CSV بسیار کارآمد نیستند: باید کار زیادی برای خواندن داده‌ها در R انجام دهید. در این فصل، در مورد یک جایگزین قدرتمند یاد خواهید گرفت: [فرمت parquet](https://parquet.apache.org/)، یک فرمت مبتنی بر استانداردهای باز که به طور گسترده توسط سیستم‌های داده بزرگ استفاده می‌شود.

ما فایل‌های parquet را با [Apache Arrow](https://arrow.apache.org) جفت خواهیم کرد، یک جعبه ابزار چندزبانه که برای تجزیه و تحلیل کارآمد و انتقال مجموعه داده‌های بزرگ طراحی شده است. ما از Apache Arrow از طریق [بسته arrow](https://arrow.apache.org/docs/r/) استفاده خواهیم کرد، که یک backend dplyr فراهم می‌کند که به شما اجازه می‌دهد مجموعه داده‌های بزرگتر از حافظه را با استفاده از نحو آشنای dplyr تجزیه و تحلیل کنید. به عنوان یک مزیت اضافی، arrow بسیار سریع است: نمونه‌هایی را بعداً در این فصل خواهید دید.

هم arrow و هم dbplyr backend های dplyr را فراهم می‌کنند، بنابراین ممکن است تعجب کنید که چه زمانی از هر کدام استفاده کنید. در بسیاری از موارد، این انتخاب برای شما انجام شده است، زیرا داده‌ها قبلاً در یک پایگاه داده یا در فایل‌های parquet قرار دارند و شما می‌خواهید همان‌طور که هست با آن کار کنید. اما اگر با داده‌های خودتان شروع می‌کنید (شاید فایل‌های CSV)، می‌توانید آن را در یک پایگاه داده بارگذاری کنید یا به parquet تبدیل کنید. به طور کلی، سخت است بدانید چه چیزی بهترین کار را انجام می‌دهد، بنابراین در مراحل اولیه تجزیه و تحلیل خود ما شما را تشویق می‌کنیم که هر دو را امتحان کنید و آنچه را که برای شما بهتر کار می‌کند انتخاب کنید.

(تشکر فراوان از Danielle Navarro که نسخه اولیه این فصل را ارائه کرد.)

### ۱.۱.۲۲ پیش‌نیازها

در این فصل، به استفاده از tidyverse، به ویژه dplyr، ادامه خواهیم داد، اما آن را با بسته **arrow** جفت خواهیم کرد که به طور خاص برای کار با داده‌های بزرگ طراحی شده است.

```{r setup}
library(tidyverse)
library(arrow)
```

بعداً در این فصل، چند ارتباط بین arrow و duckdb را نیز خواهیم دید، بنابراین به dbplyr و duckdb نیز نیاز خواهیم داشت.

```{r}
library(dbplyr, warn.conflicts = FALSE)
library(duckdb)
```

## ۲.۲۲ دریافت داده‌ها

با دریافت یک مجموعه داده شایسته این ابزارها شروع می‌کنیم: مجموعه داده‌ای از امانت کتاب‌ها از کتابخانه‌های عمومی سیاتل، که به صورت آنلاین در [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6) در دسترس است. این مجموعه داده شامل ۴۱,۳۸۹,۴۶۵ سطر است که به شما می‌گوید هر کتاب چند بار در هر ماه از آوریل ۲۰۰۵ تا اکتبر ۲۰۲۲ امانت گرفته شده است.

کد زیر یک کپی ذخیره شده از داده‌ها را برای شما دریافت می‌کند. داده‌ها یک فایل CSV به حجم ۹ گیگابایت است، بنابراین دانلود آن کمی زمان خواهد برد. ما به شدت توصیه می‌کنیم از `curl::multi_download()` برای دریافت فایل‌های بسیار بزرگ استفاده کنید زیرا دقیقاً برای این منظور ساخته شده است: به شما یک نوار پیشرفت می‌دهد و می‌تواند در صورت قطع شدن، دانلود را از سر بگیرد.

```{r}
dir.create("data", showWarnings = FALSE)

curl::multi_download(
  "https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv",
  "data/seattle-library-checkouts.csv",
  resume = TRUE
)
```

## ۳.۲۲ باز کردن یک مجموعه داده

بیایید با نگاهی به داده‌ها شروع کنیم. با حجم ۹ گیگابایت، این فایل به اندازه کافی بزرگ است که احتمالاً نمی‌خواهیم تمام آن را در حافظه بارگذاری کنیم. یک قانون کلی خوب این است که معمولاً می‌خواهید حداقل دو برابر حافظه به اندازه داده‌ها داشته باشید، و بسیاری از لپ‌تاپ‌ها حداکثر ۱۶ گیگابایت دارند. این بدان معناست که می‌خواهیم از `read_csv()` اجتناب کنیم و در عوض از `arrow::open_dataset()` استفاده کنیم:

```{r}
seattle_csv <- open_dataset(
  sources = "data/seattle-library-checkouts.csv", 
  col_types = schema(ISBN = string()),
  format = "csv"
)
```

وقتی این کد اجرا می‌شود چه اتفاقی می‌افتد؟ `open_dataset()` چند هزار سطر را اسکن خواهد کرد تا ساختار مجموعه داده را متوجه شود. ستون `ISBN` برای ۸۰,۰۰۰ سطر اول حاوی مقادیر خالی است، بنابراین باید نوع ستون را مشخص کنیم تا به arrow کمک کنیم ساختار داده را تشخیص دهد. پس از اینکه داده‌ها توسط `open_dataset()` اسکن شدند، آنچه را که یافته ثبت می‌کند و متوقف می‌شود؛ فقط زمانی که به طور خاص آن‌ها را درخواست کنید، سطرهای بیشتری را خواهد خواند. این فراداده همان چیزی است که اگر `seattle_csv` را چاپ کنیم می‌بینیم:

```{r}
seattle_csv
```

اولین خط در خروجی به شما می‌گوید که `seattle_csv` به صورت محلی روی دیسک به عنوان یک فایل CSV واحد ذخیره شده است؛ فقط در صورت نیاز در حافظه بارگذاری خواهد شد. بقیه خروجی نوع ستونی که arrow برای هر ستون استنباط کرده است را به شما می‌گوید.

می‌توانیم با `glimpse()` ببینیم که واقعاً چه چیزی در آن است. این نشان می‌دهد که تقریباً ۴۱ میلیون سطر و ۱۲ ستون وجود دارد و چند مقدار را به ما نشان می‌دهد.

```{r}
seattle_csv |> glimpse()
```

می‌توانیم شروع به استفاده از این مجموعه داده با افعال dplyr کنیم، با استفاده از `collect()` برای مجبور کردن arrow به انجام محاسبه و بازگرداندن برخی داده‌ها. به عنوان مثال، این کد تعداد کل امانت‌ها را در هر سال به ما می‌گوید:

```{r}
seattle_csv |> 
  group_by(CheckoutYear) |> 
  summarise(Checkouts = sum(Checkouts)) |> 
  arrange(CheckoutYear) |> 
  collect()
```

به لطف arrow، این کد صرف نظر از اینکه مجموعه داده اصلی چقدر بزرگ است، کار می‌کند. اما در حال حاضر نسبتاً کند است: در کامپیوتر Hadley، اجرای آن حدود ۱۰ ثانیه طول کشید. این با توجه به حجم داده‌هایی که داریم وحشتناک نیست، اما می‌توانیم آن را با تغییر به یک فرمت بهتر بسیار سریع‌تر کنیم.

## ۴.۲۲ فرمت parquet

برای آسان‌تر کردن کار با این داده‌ها، بیایید به فرمت فایل parquet تغییر دهیم و آن را به چندین فایل تقسیم کنیم. بخش‌های زیر ابتدا شما را با parquet و پارتیشن‌بندی آشنا می‌کنند، و سپس آنچه یاد گرفتیم را در مورد داده‌های کتابخانه سیاتل به کار می‌بریم.

### ۱.۴.۲۲ مزایای parquet

مانند CSV، parquet برای داده‌های مستطیلی استفاده می‌شود، اما به جای اینکه یک فرمت متنی باشد که می‌توانید با هر ویرایشگر فایل بخوانید، یک فرمت باینری سفارشی است که به طور خاص برای نیازهای داده‌های بزرگ طراحی شده است. این بدان معنی است که:

- فایل‌های Parquet معمولاً کوچک‌تر از فایل CSV معادل هستند. Parquet بر [رمزگذاری‌های کارآمد](https://parquet.apache.org/docs/file-format/data-pages/encodings/) تکیه می‌کند تا اندازه فایل را پایین نگه دارد، و از فشرده‌سازی فایل پشتیبانی می‌کند. این کمک می‌کند که فایل‌های parquet سریع باشند زیرا داده کمتری برای انتقال از دیسک به حافظه وجود دارد.

- فایل‌های Parquet یک سیستم نوع غنی دارند. همانطور که در فصل ۷ در مورد آن صحبت کردیم، یک فایل CSV هیچ اطلاعاتی در مورد انواع ستون‌ها ارائه نمی‌دهد. به عنوان مثال، یک خواننده CSV باید حدس بزند که آیا `"08-10-2022"` باید به عنوان یک رشته یا یک تاریخ تجزیه شود. در مقابل، فایل‌های parquet داده‌ها را به روشی ذخیره می‌کنند که نوع را همراه با داده ثبت می‌کند.

- فایل‌های Parquet "ستون-محور" هستند. این بدان معناست که آن‌ها ستون به ستون سازماندهی می‌شوند، بسیار شبیه به data frame R. این معمولاً منجر به عملکرد بهتر برای وظایف تجزیه و تحلیل داده در مقایسه با فایل‌های CSV می‌شود که سطر به سطر سازماندهی می‌شوند.

- فایل‌های Parquet "تکه‌بندی شده" هستند، که امکان کار بر روی بخش‌های مختلف فایل را به طور همزمان فراهم می‌کند و اگر خوش‌شانس باشید، امکان رد کردن برخی تکه‌ها را کاملاً فراهم می‌کند.

یک نقطه ضعف اصلی برای فایل‌های parquet وجود دارد: آن‌ها دیگر "قابل خواندن توسط انسان" نیستند، یعنی اگر با استفاده از `readr::read_file()` به یک فایل parquet نگاه کنید، فقط یک سری مزخرفات خواهید دید.

### ۲.۴.۲۲ پارتیشن‌بندی

همانطور که مجموعه داده‌ها بزرگتر و بزرگتر می‌شوند، ذخیره همه داده‌ها در یک فایل واحد به طور فزاینده‌ای دردناک می‌شود و اغلب مفید است که مجموعه داده‌های بزرگ را در چندین فایل تقسیم کنید. هنگامی که این ساختاردهی به صورت هوشمندانه انجام می‌شود، این استراتژی می‌تواند منجر به بهبود قابل توجهی در عملکرد شود زیرا بسیاری از تجزیه و تحلیل‌ها فقط به زیرمجموعه‌ای از فایل‌ها نیاز دارند.

هیچ قانون سخت و سریعی در مورد نحوه پارتیشن‌بندی مجموعه داده شما وجود ندارد: نتایج به داده‌ها، الگوهای دسترسی و سیستم‌هایی که داده‌ها را می‌خوانند بستگی دارد. احتمالاً باید قبل از اینکه پارتیشن‌بندی ایده‌آل برای موقعیت خود را پیدا کنید، کمی آزمایش انجام دهید. به عنوان یک راهنمای تقریبی، arrow پیشنهاد می‌کند که از فایل‌های کوچک‌تر از ۲۰ مگابایت و بزرگتر از ۲ گیگابایت اجتناب کنید و از پارتیشن‌هایی که بیش از ۱۰,۰۰۰ فایل تولید می‌کنند اجتناب کنید. همچنین باید سعی کنید بر اساس متغیرهایی که بر اساس آن‌ها فیلتر می‌کنید پارتیشن‌بندی کنید؛ همانطور که به زودی خواهید دید، این به arrow اجازه می‌دهد با خواندن فقط فایل‌های مربوطه، کار زیادی را رد کند.

### ۳.۴.۲۲ بازنویسی داده‌های کتابخانه سیاتل

بیایید این ایده‌ها را در مورد داده‌های کتابخانه سیاتل اعمال کنیم تا ببینیم در عمل چگونه عمل می‌کنند. قرار است بر اساس `CheckoutYear` پارتیشن‌بندی کنیم، زیرا احتمالاً برخی تجزیه و تحلیل‌ها فقط می‌خواهند به داده‌های اخیر نگاه کنند و پارتیشن‌بندی بر اساس سال ۱۸ تکه با اندازه معقول ایجاد می‌کند.

برای بازنویسی داده‌ها، پارتیشن را با استفاده از `dplyr::group_by()` تعریف می‌کنیم و سپس پارتیشن‌ها را با `arrow::write_dataset()` در یک دایرکتوری ذخیره می‌کنیم. `write_dataset()` دو آرگومان مهم دارد: یک دایرکتوری که در آن فایل‌ها را ایجاد خواهیم کرد و فرمتی که استفاده خواهیم کرد.

```{r}
pq_path <- "data/seattle-library-checkouts"
```

```{r}
seattle_csv |>
  group_by(CheckoutYear) |>
  write_dataset(path = pq_path, format = "parquet")
```

اجرای این کار حدود یک دقیقه طول می‌کشد؛ همانطور که به زودی خواهیم دید این یک سرمایه‌گذاری اولیه است که با سریع‌تر کردن عملیات آینده بسیار بسیار بازمی‌گردد.

بیایید نگاهی به آنچه تازه تولید کرده‌ایم بیندازیم:

```{r}
tibble(
  files = list.files(pq_path, recursive = TRUE),
  size_MB = file.size(file.path(pq_path, files)) / 1024^2
)
```

فایل CSV تک ۹ گیگابایتی ما به ۱۸ فایل parquet بازنویسی شده است. نام فایل‌ها از قرارداد "خودتوصیف‌گر" استفاده شده توسط پروژه [Apache Hive](https://hive.apache.org) استفاده می‌کنند. پارتیشن‌های به سبک Hive پوشه‌ها را با قرارداد "key=value" نام‌گذاری می‌کنند، بنابراین همانطور که ممکن است حدس بزنید، دایرکتوری `CheckoutYear=2005` حاوی تمام داده‌هایی است که `CheckoutYear` آن ۲۰۰۵ است. هر فایل بین ۱۰۰ تا ۳۰۰ مگابایت است و حجم کل اکنون حدود ۴ گیگابایت است، کمی بیش از نصف اندازه فایل CSV اصلی. این همانطور که انتظار داریم زیرا parquet یک فرمت بسیار کارآمدتر است.

## ۵.۲۲ استفاده از dplyr با arrow

اکنون که این فایل‌های parquet را ایجاد کرده‌ایم، باید دوباره آن‌ها را بخوانیم. دوباره از `open_dataset()` استفاده می‌کنیم، اما این بار یک دایرکتوری به آن می‌دهیم:

```{r}
seattle_pq <- open_dataset(pq_path)
```

اکنون می‌توانیم پایپلاین dplyr خود را بنویسیم. به عنوان مثال، می‌توانیم تعداد کل کتاب‌های امانت گرفته شده را در هر ماه برای پنج سال گذشته بشماریم:

```{r}
query <- seattle_pq |> 
  filter(CheckoutYear >= 2018, MaterialType == "BOOK") |>
  group_by(CheckoutYear, CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(CheckoutYear, CheckoutMonth)
```

نوشتن کد dplyr برای داده‌های arrow از نظر مفهومی شبیه به dbplyr است: کد dplyr می‌نویسید که به طور خودکار به یک پرس‌وجو تبدیل می‌شود که کتابخانه Apache Arrow C++ آن را درک می‌کند، که سپس هنگامی که `collect()` را فراخوانی می‌کنید اجرا می‌شود. اگر شی `query` را چاپ کنیم می‌توانیم اطلاعات کمی در مورد آنچه انتظار داریم Arrow هنگام اجرا برگرداند ببینیم:

```{r}
query
```

و می‌توانیم نتایج را با فراخوانی `collect()` دریافت کنیم:

```{r}
query |> collect()
```

مانند dbplyr، arrow فقط برخی از عبارات R را درک می‌کند، بنابراین ممکن است نتوانید دقیقاً همان کدی را که معمولاً می‌نویسید بنویسید. با این حال، لیست عملیات و توابع پشتیبانی شده نسبتاً گسترده است و به رشد ادامه می‌دهد؛ لیست کاملی از توابع در حال حاضر پشتیبانی شده را در `?acero` پیدا کنید.

### ۱.۵.۲۲ عملکرد

بیایید نگاهی سریع به تأثیر عملکرد تغییر از CSV به parquet بیندازیم. اول، بیایید زمان‌بندی کنیم که چقدر طول می‌کشد تا تعداد کتاب‌های امانت گرفته شده در هر ماه از سال ۲۰۲۱ را محاسبه کنیم، زمانی که داده‌ها به عنوان یک csv بزرگ واحد ذخیره شده‌اند:

```{r}
seattle_csv |> 
  filter(CheckoutYear == 2021, MaterialType == "BOOK") |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |> 
  system.time()
```

اکنون بیایید از نسخه جدید مجموعه داده خود استفاده کنیم که در آن داده‌های امانت کتابخانه سیاتل به ۱۸ فایل parquet کوچک‌تر پارتیشن‌بندی شده‌اند:

```{r}
seattle_pq |> 
  filter(CheckoutYear == 2021, MaterialType == "BOOK") |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |> 
  system.time()
```

افزایش سرعت تقریباً ۱۰۰ برابری در عملکرد به دو عامل نسبت داده می‌شود: پارتیشن‌بندی چند فایلی، و فرمت فایل‌های فردی:

- پارتیشن‌بندی عملکرد را بهبود می‌بخشد زیرا این پرس‌وجو از `CheckoutYear == 2021` برای فیلتر کردن داده‌ها استفاده می‌کند، و arrow به اندازه کافی هوشمند است که تشخیص دهد فقط نیاز دارد ۱ از ۱۸ فایل parquet را بخواند.
- فرمت parquet عملکرد را با ذخیره داده‌ها در یک فرمت باینری که می‌تواند مستقیماً در حافظه خوانده شود، بهبود می‌بخشد. فرمت ستون-محور و فراداده غنی به این معنی است که arrow فقط نیاز دارد چهار ستونی که واقعاً در پرس‌وجو استفاده می‌شوند را بخواند (`CheckoutYear`، `MaterialType`، `CheckoutMonth` و `Checkouts`).

این تفاوت عظیم در عملکرد دلیل آن است که تبدیل CSV های بزرگ به parquet به صرفه است!

### ۲.۵.۲۲ استفاده از duckdb با arrow

یک مزیت آخر parquet و arrow وجود دارد --- تبدیل یک مجموعه داده arrow به یک پایگاه داده DuckDB با فراخوانی `arrow::to_duckdb()` بسیار آسان است:

```{r}
seattle_pq |> 
  to_duckdb() |>
  filter(CheckoutYear >= 2018, MaterialType == "BOOK") |>
  group_by(CheckoutYear) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutYear)) |>
  collect()
```

نکته جالب در مورد `to_duckdb()` این است که انتقال شامل هیچ کپی حافظه‌ای نمی‌شود و به اهداف اکوسیستم arrow صحبت می‌کند: امکان انتقال یکپارچه از یک محیط محاسباتی به محیط دیگر.

### ۳.۵.۲۲ تمرین‌ها

۱. محبوب‌ترین کتاب را در هر سال پیدا کنید.
۲. کدام نویسنده بیشترین کتاب‌ها را در سیستم کتابخانه سیاتل دارد؟
۳. امانت کتاب‌ها در مقابل کتاب‌های الکترونیکی در ۱۰ سال گذشته چگونه تغییر کرده است؟

## ۶.۲۲ خلاصه

در این فصل، طعمی از بسته arrow به شما داده شد، که یک backend dplyr برای کار با مجموعه داده‌های بزرگ روی دیسک فراهم می‌کند. می‌تواند با فایل‌های CSV کار کند، و اگر داده‌های خود را به parquet تبدیل کنید بسیار بسیار سریع‌تر است. Parquet یک فرمت داده باینری است که به طور خاص برای تجزیه و تحلیل داده بر روی کامپیوترهای مدرن طراحی شده است. ابزارهای بسیار کمتری می‌توانند با فایل‌های parquet در مقایسه با CSV کار کنند، اما ساختار پارتیشن‌بندی شده، فشرده و ستون-محور آن، تجزیه و تحلیل آن را بسیار کارآمدتر می‌کند.

در مرحله بعد در مورد اولین منبع داده غیرمستطیلی خود یاد خواهید گرفت، که با استفاده از ابزارهای ارائه شده توسط بسته tidyr با آن کار خواهید کرد. ما بر روی داده‌هایی که از فایل‌های JSON می‌آیند تمرکز خواهیم کرد، اما اصول کلی صرف نظر از منبع آن به داده‌های درختی اعمال می‌شوند.
